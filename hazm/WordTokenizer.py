# coding: utf-8

from __future__ import unicode_literals
import re, codecs
from .utils import words_list, default_words, default_verbs
from nltk.tokenize.api import TokenizerI


class WordTokenizer(TokenizerI):
	"""
	>>> tokenizer = WordTokenizer()
	>>> tokenizer.tokenize('Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ (Ø®ÛŒÙ„ÛŒ) Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù†ÛŒØ³Øª!!!')
	['Ø§ÛŒÙ†', 'Ø¬Ù…Ù„Ù‡', '(', 'Ø®ÛŒÙ„ÛŒ', ')', 'Ù¾ÛŒÚ†ÛŒØ¯Ù‡', 'Ù†ÛŒØ³Øª', '!!!']

	>>> tokenizer.tokenize('Ù†Ø³Ø®Ù‡ 0.5 Ø¯Ø± Ø³Ø§Ø¹Øª 22:00 ØªÙ‡Ø±Ø§Ù†ØŒ1396')
	['Ù†Ø³Ø®Ù‡', '0.5', 'Ø¯Ø±', 'Ø³Ø§Ø¹Øª', '22:00', 'ØªÙ‡Ø±Ø§Ù†', 'ØŒ', '1396']

	>>> tokenizer = WordTokenizer(join_verb_parts=False)
	>>> print(' '.join(tokenizer.tokenize('Ø³Ù„Ø§Ù….')))
	Ø³Ù„Ø§Ù… .

	>>> tokenizer = WordTokenizer(join_verb_parts=False, replace_links=True)
	>>> print(' '.join(tokenizer.tokenize('Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ https://t.co/tZOurPSXzi https://t.co/vtJtwsRebP')))
	Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ LINK LINK

	>>> tokenizer = WordTokenizer(join_verb_parts=False, replace_IDs=True, replace_numbers=True)
	>>> print(' '.join(tokenizer.tokenize('Ø²Ù„Ø²Ù„Ù‡ Û´.Û¸ Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† @bourse24ir')))
	Ø²Ù„Ø²Ù„Ù‡ NUMF Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† ID

	>>> tokenizer = WordTokenizer(join_verb_parts=False, replace_hashtags=True, replace_numbers=True, separate_emoji=True)
	>>> print(' '.join(tokenizer.tokenize('ðŸ“Ø¹Ø±Ø¶Ù‡ Ø¨Ù„ÙˆÚ© 17 Ø¯Ø±ØµØ¯ÛŒ #Ù‡Ø§ÛŒ_ÙˆØ¨ Ø¨Ù‡ Ù‚ÛŒÙ…Øª')))
	ðŸ“ Ø¹Ø±Ø¶Ù‡ Ø¨Ù„ÙˆÚ© NUM2 Ø¯Ø±ØµØ¯ÛŒ TAG Ù‡Ø§ÛŒ ÙˆØ¨ Ø¨Ù‡ Ù‚ÛŒÙ…Øª

	>>> tokenizer = WordTokenizer(join_verb_parts=False, separate_emoji=True)
	>>> print(' '.join(tokenizer.tokenize('Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ðŸ˜‚ðŸ˜‚ðŸ˜‚')))
	Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ðŸ˜‚ ðŸ˜‚ ðŸ˜‚
	"""

	def __init__(self, words_file=default_words, verbs_file=default_verbs, join_verb_parts=True, separate_emoji=False, replace_links=False, replace_IDs=False, replace_emails=False, replace_numbers=False, replace_hashtags=False):
		self._join_verb_parts = join_verb_parts
		self.separate_emoji = separate_emoji
		self.replace_links = replace_links
		self.replace_IDs = replace_IDs
		self.replace_emails = replace_emails
		self.replace_numbers = replace_numbers
		self.replace_hashtags = replace_hashtags

		self.pattern = re.compile(r'([ØŸ!\?]+|\d[\d\.:/\\]+|[:\.ØŒØ›Â»\]\)\}"Â«\[\(\{])')  # TODO \d
		self.emoji_pattern = re.compile(u"["
										u"\U0001F600-\U0001F64F"  # emoticons
										u"\U0001F300-\U0001F5FF"  # symbols & pictographs
										u"\U0001F4CC\U0001F4CD"  # pushpin & round pushpin
										"]", flags=re.UNICODE)
		self.emoji_repl = r'\g<0> '
		self.id_pattern = re.compile(r'([^\w\._]+)(@[\w_]+)')
		self.id_repl = r'\1ID'
		self.link_pattern = re.compile(r'((https?|ftp):\/\/)?(?<!@)([wW]{3}\.)?(([\w-]+)(\.(\w){2,})+([-\w@:%_\+\/~#?&]+)?)')
		self.link_repl = r'LINK'
		self.email_pattern = re.compile(r'[a-zA-Z0-9\._\+-]+@([a-zA-Z0-9-]+\.)+[A-Za-z]{2,}')
		self.email_repl = r'EMAIL'
		self.number_int_pattern = re.compile(r'([^\.,\w]+)([\dÛ°-Û¹]+)([^\.,\w]+)')
		self.number_int_repl = lambda m: m.group(1) + 'NUM'+ str(len(m.group(2))) + m.group(3)
		self.number_float_pattern = re.compile(r'([^,\w]+)([\dÛ°-Û¹,]+[\.Ù«]{1}[\dÛ°-Û¹]+)([^,\w]+)')
		self.number_float_repl = r'\1NUMF\3'
		self.hashtag_pattern = re.compile(r'\#([\S]+)')
		# NOTE: python2.7 does not support unicodes with \w  Example: r'\#([\w\_]+)'

		self.hashtag_repl = lambda m: 'TAG ' + m.group(1).replace('_', ' ')

		self.words = {item[0]: (item[1], item[2]) for item in words_list(default_words)}

		if join_verb_parts:
			self.after_verbs = set([
				'Ø§Ù…', 'Ø§ÛŒ', 'Ø§Ø³Øª', 'Ø§ÛŒÙ…', 'Ø§ÛŒØ¯', 'Ø§Ù†Ø¯', 'Ø¨ÙˆØ¯Ù…', 'Ø¨ÙˆØ¯ÛŒ', 'Ø¨ÙˆØ¯', 'Ø¨ÙˆØ¯ÛŒÙ…', 'Ø¨ÙˆØ¯ÛŒØ¯', 'Ø¨ÙˆØ¯Ù†Ø¯', 'Ø¨Ø§Ø´Ù…', 'Ø¨Ø§Ø´ÛŒ', 'Ø¨Ø§Ø´Ø¯', 'Ø¨Ø§Ø´ÛŒÙ…', 'Ø¨Ø§Ø´ÛŒØ¯', 'Ø¨Ø§Ø´Ù†Ø¯',
				'Ø´Ø¯Ù‡_Ø§Ù…', 'Ø´Ø¯Ù‡_Ø§ÛŒ', 'Ø´Ø¯Ù‡_Ø§Ø³Øª', 'Ø´Ø¯Ù‡_Ø§ÛŒÙ…', 'Ø´Ø¯Ù‡_Ø§ÛŒØ¯', 'Ø´Ø¯Ù‡_Ø§Ù†Ø¯', 'Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù…', 'Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒ', 'Ø´Ø¯Ù‡_Ø¨ÙˆØ¯', 'Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒÙ…', 'Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒØ¯', 'Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù†Ø¯', 'Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù…', 'Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒ', 'Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ø¯', 'Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒÙ…', 'Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒØ¯', 'Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù†Ø¯',
				'Ù†Ø´Ø¯Ù‡_Ø§Ù…', 'Ù†Ø´Ø¯Ù‡_Ø§ÛŒ', 'Ù†Ø´Ø¯Ù‡_Ø§Ø³Øª', 'Ù†Ø´Ø¯Ù‡_Ø§ÛŒÙ…', 'Ù†Ø´Ø¯Ù‡_Ø§ÛŒØ¯', 'Ù†Ø´Ø¯Ù‡_Ø§Ù†Ø¯', 'Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù…', 'Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒ', 'Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯', 'Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒÙ…', 'Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒØ¯', 'Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù†Ø¯', 'Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù…', 'Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒ', 'Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ø¯', 'Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒÙ…', 'Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒØ¯', 'Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù†Ø¯',
				'Ø´ÙˆÙ…', 'Ø´ÙˆÛŒ', 'Ø´ÙˆØ¯', 'Ø´ÙˆÛŒÙ…', 'Ø´ÙˆÛŒØ¯', 'Ø´ÙˆÙ†Ø¯', 'Ø´Ø¯Ù…', 'Ø´Ø¯ÛŒ', 'Ø´Ø¯', 'Ø´Ø¯ÛŒÙ…', 'Ø´Ø¯ÛŒØ¯', 'Ø´Ø¯Ù†Ø¯',
				'Ù†Ø´ÙˆÙ…', 'Ù†Ø´ÙˆÛŒ', 'Ù†Ø´ÙˆØ¯', 'Ù†Ø´ÙˆÛŒÙ…', 'Ù†Ø´ÙˆÛŒØ¯', 'Ù†Ø´ÙˆÙ†Ø¯', 'Ù†Ø´Ø¯Ù…', 'Ù†Ø´Ø¯ÛŒ', 'Ù†Ø´Ø¯', 'Ù†Ø´Ø¯ÛŒÙ…', 'Ù†Ø´Ø¯ÛŒØ¯', 'Ù†Ø´Ø¯Ù†Ø¯',
				'Ù…ÛŒâ€ŒØ´ÙˆÙ…', 'Ù…ÛŒâ€ŒØ´ÙˆÛŒ', 'Ù…ÛŒâ€ŒØ´ÙˆØ¯', 'Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…', 'Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯', 'Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯', 'Ù…ÛŒâ€ŒØ´Ø¯Ù…', 'Ù…ÛŒâ€ŒØ´Ø¯ÛŒ', 'Ù…ÛŒâ€ŒØ´Ø¯', 'Ù…ÛŒâ€ŒØ´Ø¯ÛŒÙ…', 'Ù…ÛŒâ€ŒØ´Ø¯ÛŒØ¯', 'Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯',
				'Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ…', 'Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒ', 'Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯', 'Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…', 'Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯', 'Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯', 'Ù†Ù…ÛŒâ€ŒØ´Ø¯Ù…', 'Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒ', 'Ù†Ù…ÛŒâ€ŒØ´Ø¯', 'Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒÙ…', 'Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒØ¯', 'Ù†Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯',
				'Ø®ÙˆØ§Ù‡Ù…_Ø´Ø¯', 'Ø®ÙˆØ§Ù‡ÛŒ_Ø´Ø¯', 'Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯', 'Ø®ÙˆØ§Ù‡ÛŒÙ…_Ø´Ø¯', 'Ø®ÙˆØ§Ù‡ÛŒØ¯_Ø´Ø¯', 'Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯',
				'Ù†Ø®ÙˆØ§Ù‡Ù…_Ø´Ø¯', 'Ù†Ø®ÙˆØ§Ù‡ÛŒ_Ø´Ø¯', 'Ù†Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯', 'Ù†Ø®ÙˆØ§Ù‡ÛŒÙ…_Ø´Ø¯', 'Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯_Ø´Ø¯', 'Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯',
			])

			self.before_verbs = set([
				'Ø®ÙˆØ§Ù‡Ù…', 'Ø®ÙˆØ§Ù‡ÛŒ', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ø®ÙˆØ§Ù‡ÛŒÙ…', 'Ø®ÙˆØ§Ù‡ÛŒØ¯', 'Ø®ÙˆØ§Ù‡Ù†Ø¯',
				'Ù†Ø®ÙˆØ§Ù‡Ù…', 'Ù†Ø®ÙˆØ§Ù‡ÛŒ', 'Ù†Ø®ÙˆØ§Ù‡Ø¯', 'Ù†Ø®ÙˆØ§Ù‡ÛŒÙ…', 'Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯', 'Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯'
			])

			with codecs.open(verbs_file, encoding='utf8') as verbs_file:
				self.verbs = list(reversed([verb.strip() for verb in verbs_file if verb]))
				self.bons = set([verb.split('#')[0] for verb in self.verbs])
				self.verbe = set([bon +'Ù‡' for bon in self.bons] + ['Ù†'+ bon +'Ù‡' for bon in self.bons])

	def tokenize(self, text):

		if self.separate_emoji:
			text = self.emoji_pattern.sub(self.emoji_repl, text)
		if self.replace_links:
			text = self.link_pattern.sub(self.link_repl, text)
		if self.replace_IDs:
			text = self.id_pattern.sub(self.id_repl, text)
		if self.replace_emails:
			text = self.email_pattern.sub(self.email_repl, text)
		if self.replace_hashtags:
			text = self.hashtag_pattern.sub(self.hashtag_repl, text)
		if self.replace_numbers:
			text = self.number_int_pattern.sub(self.number_int_repl, text)
			text = self.number_float_pattern.sub(self.number_float_repl, text)

		text = self.pattern.sub(r' \1 ', text.replace('\n', ' ').replace('\t', ' '))

		tokens = [word for word in text.split(' ') if word]
		if self._join_verb_parts:
			tokens = self.join_verb_parts(tokens)
		return tokens

	def join_verb_parts(self, tokens):
		"""
		>>> tokenizer = WordTokenizer()
		>>> tokenizer.join_verb_parts(['Ø®ÙˆØ§Ù‡Ø¯', 'Ø±ÙØª'])
		['Ø®ÙˆØ§Ù‡Ø¯_Ø±ÙØª']
		>>> tokenizer.join_verb_parts(['Ø±ÙØªÙ‡', 'Ø§Ø³Øª'])
		['Ø±ÙØªÙ‡_Ø§Ø³Øª']
		>>> tokenizer.join_verb_parts(['Ú¯ÙØªÙ‡', 'Ø´Ø¯Ù‡', 'Ø§Ø³Øª'])
		['Ú¯ÙØªÙ‡_Ø´Ø¯Ù‡_Ø§Ø³Øª']
		>>> tokenizer.join_verb_parts(['Ú¯ÙØªÙ‡', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ø´Ø¯'])
		['Ú¯ÙØªÙ‡_Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯']
		>>> tokenizer.join_verb_parts(['Ø®Ø³ØªÙ‡', 'Ø´Ø¯ÛŒØ¯'])
		['Ø®Ø³ØªÙ‡', 'Ø´Ø¯ÛŒØ¯']
		"""

		result = ['']
		for token in reversed(tokens):
			if token in self.before_verbs or (result[-1] in self.after_verbs and token in self.verbe):
				result[-1] = token +'_'+ result[-1]
			else:
				result.append(token)
		return list(reversed(result[1:]))
