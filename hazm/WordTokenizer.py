# coding: utf-8

"""Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ø´Ø§Ù…Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ùˆ ØªÙˆØ§Ø¨Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§ØªÙ Ù…ØªÙ† Ø§Ø³Øª. 

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„Ø§ØªØŒ Ø§Ø² ØªØ§Ø¨Ø¹ [SentenceTokenizer()][hazm.SentenceTokenizer] Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.
"""

from __future__ import unicode_literals
import re
import codecs
from .utils import words_list, default_words, default_verbs
from nltk.tokenize.api import TokenizerI


class WordTokenizer(TokenizerI):
    """Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø´Ø§Ù…Ù„ ØªÙˆØ§Ø¨Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§ØªÙ Ù…ØªÙ† Ø§Ø³Øª.

    Args:
            words_file (str, optional): Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø­Ø§ÙˆÛŒ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª.
                    Ù‡Ø¶Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ù†Ø¸ÙˆØ± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³ØªØ› Ø¨Ø§
                    Ø§ÛŒÙ† Ø­Ø§Ù„ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙØ§ÛŒÙ„ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ø¢Ú¯Ø§Ù‡ÛŒ Ø§Ø²
                    Ø³Ø§Ø®ØªØ§Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.

            verbs_file (str, optional): Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø­Ø§ÙˆÛŒ Ø§ÙØ¹Ø§Ù„. 
                    Ù‡Ø¶Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ù†Ø¸ÙˆØ± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³ØªØ› Ø¨Ø§
                    Ø§ÛŒÙ† Ø­Ø§Ù„ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙØ§ÛŒÙ„ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ø¢Ú¯Ø§Ù‡ÛŒ Ø§Ø²
                    Ø³Ø§Ø®ØªØ§Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.

            separate_emoji (bool, optional): Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø§Ù…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ ÛŒÚ© ÙØ§ØµÙ„Ù‡ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            replace_links (bool, optional): Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù„Ù…Ù‡Ù” `LINK` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            replace_IDs (bool, optional): Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù„Ù…Ù‡Ù” `ID` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            replace_emails (bool, optional): Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø§ Ú©Ù„Ù…Ù‡Ù” `EMAILâ€` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            replace_numbers (bool, optional): Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø§`NUMF` Ùˆ Ø§Ø¹Ø¯Ø§Ø¯ ØµØ­ÛŒØ­ Ø±Ø§ Ø¨Ø§` NUM` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¯Ø± Ø§Ø¹Ø¯Ø§Ø¯ ØºÛŒØ±Ø§Ø¹Ø´Ø§Ø±ÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ù‚Ø§Ù… Ù†ÛŒØ² Ø¬Ù„ÙˆÛŒ `NUM` Ù…ÛŒâ€ŒØ¢ÛŒØ¯.
            replace_hashtags (bool, optional): Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø¹Ù„Ø§Ù…Øª `#` Ø±Ø§ Ø¨Ø§ `TAG` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """

    def __init__(self, words_file=default_words, verbs_file=default_verbs, separate_emoji=False, replace_links=False, replace_IDs=False, replace_emails=False, replace_numbers=False, replace_hashtags=False):
        self.separate_emoji = separate_emoji
        self.replace_links = replace_links
        self.replace_IDs = replace_IDs
        self.replace_emails = replace_emails
        self.replace_numbers = replace_numbers
        self.replace_hashtags = replace_hashtags

        self.pattern = re.compile(
            r'([ØŸ!\?]+|\d[\d\.:\/\\]+\d|[:\.ØŒØ›Â»\]\)\}"Â«\[\(\{])')  # TODO \d
        self.emoji_pattern = re.compile(u"["
                                        u"\U0001F600-\U0001F64F"  # emoticons
                                        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                        u"\U0001F4CC\U0001F4CD"  # other emojis
                                        "]", flags=re.UNICODE)
        self.emoji_repl = r'\g<0> '
        self.id_pattern = re.compile(r'(?<![\w\._])(@[\w_]+)')
        self.id_repl = r' ID '
        self.link_pattern = re.compile(
            r'((https?|ftp):\/\/)?(?<!@)(([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,})[-\w@:%_\.\+\/~#?=&]*')
        self.link_repl = r' LINK '
        self.email_pattern = re.compile(
            r'[a-zA-Z0-9\._\+-]+@([a-zA-Z0-9-]+\.)+[A-Za-z]{2,}')
        self.email_repl = r' EMAIL '

        # 'Ù«' is the decimal separator and 'Ù¬' is the thousands separator
        self.number_int_pattern = re.compile(
            r'\b(?<![\dÛ°-Û¹][\.Ù«Ù¬,])([\dÛ°-Û¹]+)(?![\.Ù«Ù¬,][\dÛ°-Û¹])\b')
        self.number_int_repl = lambda m: ' NUM' + str(len(m.group(1))) + ' '
        self.number_float_pattern = re.compile(
            r'\b(?<!\.)([\dÛ°-Û¹,Ù¬]+[\.Ù«Ù¬]{1}[\dÛ°-Û¹]+)\b(?!\.)')
        self.number_float_repl = r' NUMF '

        self.hashtag_pattern = re.compile(r'\#([\S]+)')
        # NOTE: python2.7 does not support unicodes with \w

        self.hashtag_repl = lambda m: 'TAG ' + m.group(1).replace('_', ' ')

        self.words = {item[0]: (item[1], item[2])
                      for item in words_list(words_file)}

        with codecs.open(verbs_file, encoding='utf8') as verbs_file:
            self.verbs = list(reversed([verb.strip()
                              for verb in verbs_file if verb]))
            self.bons = set([verb.split('#')[0] for verb in self.verbs])
            self.verbe = set([bon + 'Ù‡' for bon in self.bons] +
                             ['Ù†' + bon + 'Ù‡' for bon in self.bons])

    def tokenize(self, text):
        """ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        Examples:
                >>> tokenizer = WordTokenizer()
                >>> tokenizer.tokenize('Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ (Ø®ÛŒÙ„ÛŒ) Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù†ÛŒØ³Øª!!!')
                ['Ø§ÛŒÙ†', 'Ø¬Ù…Ù„Ù‡', '(', 'Ø®ÛŒÙ„ÛŒ', ')', 'Ù¾ÛŒÚ†ÛŒØ¯Ù‡', 'Ù†ÛŒØ³Øª', '!!!']

                >>> tokenizer.tokenize('Ù†Ø³Ø®Ù‡ 0.5 Ø¯Ø± Ø³Ø§Ø¹Øª 22:00 ØªÙ‡Ø±Ø§Ù†ØŒ1396.')
                ['Ù†Ø³Ø®Ù‡', '0.5', 'Ø¯Ø±', 'Ø³Ø§Ø¹Øª', '22:00', 'ØªÙ‡Ø±Ø§Ù†', 'ØŒ', '1396', '.']

                >>> tokenizer = WordTokenizer()
                >>> print(' '.join(tokenizer.tokenize('Ø³Ù„Ø§Ù….')))
                Ø³Ù„Ø§Ù… .

                >>> tokenizer = WordTokenizer(replace_links=True)
                >>> print(' '.join(tokenizer.tokenize('Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ https://t.co/tZOurPSXzi https://t.co/vtJtwsRebP')))
                Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ LINK LINK

                >>> tokenizer = WordTokenizer(replace_IDs=True, replace_numbers=True)
                >>> print(' '.join(tokenizer.tokenize('Ø²Ù„Ø²Ù„Ù‡ Û´.Û¸ Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† @bourse24ir')))
                Ø²Ù„Ø²Ù„Ù‡ NUMF Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† ID

                >>> tokenizer = WordTokenizer(replace_hashtags=True, replace_numbers=True, separate_emoji=True)
                >>> print(' '.join(tokenizer.tokenize('ğŸ“Ø¹Ø±Ø¶Ù‡ Ø¨Ù„ÙˆÚ© 17 Ø¯Ø±ØµØ¯ÛŒ #Ù‡Ø§ÛŒ_ÙˆØ¨ Ø¨Ù‡ Ù‚ÛŒÙ…Øª')))
                ğŸ“ Ø¹Ø±Ø¶Ù‡ Ø¨Ù„ÙˆÚ© NUM2 Ø¯Ø±ØµØ¯ÛŒ TAG Ù‡Ø§ÛŒ ÙˆØ¨ Ø¨Ù‡ Ù‚ÛŒÙ…Øª

                >>> tokenizer = WordTokenizer(separate_emoji=True)
                >>> print(' '.join(tokenizer.tokenize('Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ğŸ˜‚ğŸ˜‚ğŸ˜‚')))
                Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ğŸ˜‚ ğŸ˜‚ ğŸ˜‚

        Args:
                text (str): Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯.

        Returns:
                (List[str]): Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡.
        """

        if self.separate_emoji:
            text = self.emoji_pattern.sub(self.emoji_repl, text)
        if self.replace_emails:
            text = self.email_pattern.sub(self.email_repl, text)
        if self.replace_links:
            text = self.link_pattern.sub(self.link_repl, text)
        if self.replace_IDs:
            text = self.id_pattern.sub(self.id_repl, text)
        if self.replace_hashtags:
            text = self.hashtag_pattern.sub(self.hashtag_repl, text)
        if self.replace_numbers:
            text = self.number_int_pattern.sub(self.number_int_repl, text)
            text = self.number_float_pattern.sub(self.number_float_repl, text)

        text = self.pattern.sub(
            r' \1 ', text.replace('\n', ' ').replace('\t', ' '))

        tokens = [word for word in text.split(' ') if word]
        return tokens
