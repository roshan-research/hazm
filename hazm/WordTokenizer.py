# coding: utf-8

"""ุงู ูุงฺูู ุดุงูู ฺฉูุงุณโูุง ู ุชูุงุจุน ุจุฑุง ุงุณุชุฎุฑุงุฌ ฺฉููุงุชู ูุชู ุงุณุช. 

ุจุฑุง ุงุณุชุฎุฑุงุฌ ุฌููุงุชุ ุงุฒ ุชุงุจุน [SentenceTokenizer()][hazm.SentenceTokenizer] ุงุณุชูุงุฏู ฺฉูุฏ.
"""

from __future__ import unicode_literals
import re
import codecs
from .utils import words_list, default_words, default_verbs
from nltk.tokenize.api import TokenizerI


class WordTokenizer(TokenizerI):
    """ุงู ฺฉูุงุณ ุดุงูู ุชูุงุจุน ุจุฑุง ุงุณุชุฎุฑุงุฌ ฺฉููุงุชู ูุชู ุงุณุช.

    Args:
            words_file (str, optional): ูุณุฑ ูุงู ุญุงู ูุณุช ฺฉููุงุช.
                    ูุถู ุจู ุตูุฑุช ูพุดโูุฑุถ ูุงู ุจุฑุง ุงู ููุธูุฑ ุฏุฑ ูุธุฑ ฺฏุฑูุชู ุงุณุชุ ุจุง
                    ุงู ุญุงู ุดูุง ูโุชูุงูุฏ ูุงู ููุฑุฏูุธุฑ ุฎูุฏ ุฑุง ูุนุฑู ฺฉูุฏ. ุจุฑุง ุขฺฏุงู ุงุฒ
                    ุณุงุฎุชุงุฑ ุงู ูุงู ุจู ูุงู ูพุดโูุฑุถ ูุฑุงุฌุนู ฺฉูุฏ.

            verbs_file (str, optional): ูุณุฑ ูุงู ุญุงู ุงูุนุงู.
                    ูุถู ุจู ุตูุฑุช ูพุดโูุฑุถ ูุงู ุจุฑุง ุงู ููุธูุฑ ุฏุฑ ูุธุฑ ฺฏุฑูุชู ุงุณุชุ ุจุง
                    ุงู ุญุงู ุดูุง ูโุชูุงูุฏ ูุงู ููุฑุฏูุธุฑ ุฎูุฏ ุฑุง ูุนุฑู ฺฉูุฏ. ุจุฑุง ุขฺฏุงู ุงุฒ
                    ุณุงุฎุชุงุฑ ุงู ูุงู ุจู ูุงู ูพุดโูุฑุถ ูุฑุงุฌุนู ฺฉูุฏ.

            join_verb_parts (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ุงูุนุงู ฺูุฏุจุฎุด ุฑุง ุจุง ุฎุท ุฒุฑ ุจู ูู ูโฺุณุจุงูุฏุ ูุซูุงู ยซฺฏูุชู ุดุฏู ุงุณุชยป ุฑุง ุจู ุตูุฑุช ยซฺฏูุชู_ุดุฏู_ุงุณุชยป ุจุฑูโฺฏุฑุฏุงูุฏ.
            separate_emoji (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ุงููุฌโูุง ุฑุง ุจุง ฺฉ ูุงุตูู ุงุฒ ูู ุฌุฏุง ูโฺฉูุฏ.
            replace_links (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ููฺฉโูุง ุฑุง ุจุง ฺฉูููู `LINK` ุฌุงฺฏุฒู ูโฺฉูุฏ.
            replace_IDs (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ุดูุงุณูโูุง ุฑุง ุจุง ฺฉูููู `ID` ุฌุงฺฏุฒู ูโฺฉูุฏ.
            replace_emails (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ุขุฏุฑุณโูุง ุงูู ุฑุง ุจุง ฺฉูููู `EMAILโ` ุฌุงฺฏุฒู ูโฺฉูุฏ.
            replace_numbers (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ุงุนุฏุงุฏ ุงุนุดุงุฑ ุฑุง ุจุง`NUMF` ู ุงุนุฏุงุฏ ุตุญุญ ุฑุง ุจุง` NUM` ุฌุงฺฏุฒู ูโฺฉูุฏ. ุฏุฑ ุงุนุฏุงุฏ ุบุฑุงุนุดุงุฑุ ุชุนุฏุงุฏ ุงุฑูุงู ูุฒ ุฌูู `NUM` ูโุขุฏ.
            replace_hashtags (bool, optional): ุงฺฏุฑ `True` ุจุงุดุฏ ุนูุงูุช `#` ุฑุง ุจุง `TAG` ุฌุงฺฏุฒู ูโฺฉูุฏ.
    """

    def __init__(
        self,
        words_file=default_words,
        verbs_file=default_verbs,
        join_verb_parts=False,
        separate_emoji=False,
        replace_links=False,
        replace_IDs=False,
        replace_emails=False,
        replace_numbers=False,
        replace_hashtags=False,
    ):
        self._join_verb_parts = join_verb_parts
        self.separate_emoji = separate_emoji
        self.replace_links = replace_links
        self.replace_IDs = replace_IDs
        self.replace_emails = replace_emails
        self.replace_numbers = replace_numbers
        self.replace_hashtags = replace_hashtags

        self.pattern = re.compile(
            r'([ุ!\?]+|\d[\d\.:\/\\]+\d|[:\.ุุยป\]\)\}"ยซ\[\(\{])'
        )  # TODO \d
        self.emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F4CC\U0001F4CD"  # other emojis
            "]",
            flags=re.UNICODE,
        )
        self.emoji_repl = r"\g<0> "
        self.id_pattern = re.compile(r"(?<![\w\._])(@[\w_]+)")
        self.id_repl = r" ID "
        self.link_pattern = re.compile(
            r"((https?|ftp):\/\/)?(?<!@)(([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,})[-\w@:%_\.\+\/~#?=&]*"
        )
        self.link_repl = r" LINK "
        self.email_pattern = re.compile(
            r"[a-zA-Z0-9\._\+-]+@([a-zA-Z0-9-]+\.)+[A-Za-z]{2,}"
        )
        self.email_repl = r" EMAIL "

        # 'ูซ' is the decimal separator and 'ูฌ' is the thousands separator
        self.number_int_pattern = re.compile(
            r"\b(?<![\dฐ-น][\.ูซูฌ,])([\dฐ-น]+)(?![\.ูซูฌ,][\dฐ-น])\b"
        )
        self.number_int_repl = lambda m: " NUM" + str(len(m.group(1))) + " "
        self.number_float_pattern = re.compile(
            r"\b(?<!\.)([\dฐ-น,ูฌ]+[\.ูซูฌ]{1}[\dฐ-น]+)\b(?!\.)"
        )
        self.number_float_repl = r" NUMF "

        self.hashtag_pattern = re.compile(r"\#([\S]+)")
        # NOTE: python2.7 does not support unicodes with \w

        self.hashtag_repl = lambda m: "TAG " + m.group(1).replace("_", " ")

        self.words = {item[0]: (item[1], item[2]) for item in words_list(words_file)}

        if join_verb_parts:
            self.after_verbs = set(
                [
                    "ุงู",
                    "ุง",
                    "ุงุณุช",
                    "ุงู",
                    "ุงุฏ",
                    "ุงูุฏ",
                    "ุจูุฏู",
                    "ุจูุฏ",
                    "ุจูุฏ",
                    "ุจูุฏู",
                    "ุจูุฏุฏ",
                    "ุจูุฏูุฏ",
                    "ุจุงุดู",
                    "ุจุงุด",
                    "ุจุงุดุฏ",
                    "ุจุงุดู",
                    "ุจุงุดุฏ",
                    "ุจุงุดูุฏ",
                    "ุดุฏู_ุงู",
                    "ุดุฏู_ุง",
                    "ุดุฏู_ุงุณุช",
                    "ุดุฏู_ุงู",
                    "ุดุฏู_ุงุฏ",
                    "ุดุฏู_ุงูุฏ",
                    "ุดุฏู_ุจูุฏู",
                    "ุดุฏู_ุจูุฏ",
                    "ุดุฏู_ุจูุฏ",
                    "ุดุฏู_ุจูุฏู",
                    "ุดุฏู_ุจูุฏุฏ",
                    "ุดุฏู_ุจูุฏูุฏ",
                    "ุดุฏู_ุจุงุดู",
                    "ุดุฏู_ุจุงุด",
                    "ุดุฏู_ุจุงุดุฏ",
                    "ุดุฏู_ุจุงุดู",
                    "ุดุฏู_ุจุงุดุฏ",
                    "ุดุฏู_ุจุงุดูุฏ",
                    "ูุดุฏู_ุงู",
                    "ูุดุฏู_ุง",
                    "ูุดุฏู_ุงุณุช",
                    "ูุดุฏู_ุงู",
                    "ูุดุฏู_ุงุฏ",
                    "ูุดุฏู_ุงูุฏ",
                    "ูุดุฏู_ุจูุฏู",
                    "ูุดุฏู_ุจูุฏ",
                    "ูุดุฏู_ุจูุฏ",
                    "ูุดุฏู_ุจูุฏู",
                    "ูุดุฏู_ุจูุฏุฏ",
                    "ูุดุฏู_ุจูุฏูุฏ",
                    "ูุดุฏู_ุจุงุดู",
                    "ูุดุฏู_ุจุงุด",
                    "ูุดุฏู_ุจุงุดุฏ",
                    "ูุดุฏู_ุจุงุดู",
                    "ูุดุฏู_ุจุงุดุฏ",
                    "ูุดุฏู_ุจุงุดูุฏ",
                    "ุดูู",
                    "ุดู",
                    "ุดูุฏ",
                    "ุดูู",
                    "ุดูุฏ",
                    "ุดููุฏ",
                    "ุดุฏู",
                    "ุดุฏ",
                    "ุดุฏ",
                    "ุดุฏู",
                    "ุดุฏุฏ",
                    "ุดุฏูุฏ",
                    "ูุดูู",
                    "ูุดู",
                    "ูุดูุฏ",
                    "ูุดูู",
                    "ูุดูุฏ",
                    "ูุดููุฏ",
                    "ูุดุฏู",
                    "ูุดุฏ",
                    "ูุดุฏ",
                    "ูุดุฏู",
                    "ูุดุฏุฏ",
                    "ูุดุฏูุฏ",
                    "ูโุดูู",
                    "ูโุดู",
                    "ูโุดูุฏ",
                    "ูโุดูู",
                    "ูโุดูุฏ",
                    "ูโุดููุฏ",
                    "ูโุดุฏู",
                    "ูโุดุฏ",
                    "ูโุดุฏ",
                    "ูโุดุฏู",
                    "ูโุดุฏุฏ",
                    "ูโุดุฏูุฏ",
                    "ููโุดูู",
                    "ููโุดู",
                    "ููโุดูุฏ",
                    "ููโุดูู",
                    "ููโุดูุฏ",
                    "ููโุดููุฏ",
                    "ููโุดุฏู",
                    "ููโุดุฏ",
                    "ููโุดุฏ",
                    "ููโุดุฏู",
                    "ููโุดุฏุฏ",
                    "ููโุดุฏูุฏ",
                    "ุฎูุงูู_ุดุฏ",
                    "ุฎูุงู_ุดุฏ",
                    "ุฎูุงูุฏ_ุดุฏ",
                    "ุฎูุงูู_ุดุฏ",
                    "ุฎูุงูุฏ_ุดุฏ",
                    "ุฎูุงููุฏ_ุดุฏ",
                    "ูุฎูุงูู_ุดุฏ",
                    "ูุฎูุงู_ุดุฏ",
                    "ูุฎูุงูุฏ_ุดุฏ",
                    "ูุฎูุงูู_ุดุฏ",
                    "ูุฎูุงูุฏ_ุดุฏ",
                    "ูุฎูุงููุฏ_ุดุฏ",
                ]
            )

            self.before_verbs = set(
                [
                    "ุฎูุงูู",
                    "ุฎูุงู",
                    "ุฎูุงูุฏ",
                    "ุฎูุงูู",
                    "ุฎูุงูุฏ",
                    "ุฎูุงููุฏ",
                    "ูุฎูุงูู",
                    "ูุฎูุงู",
                    "ูุฎูุงูุฏ",
                    "ูุฎูุงูู",
                    "ูุฎูุงูุฏ",
                    "ูุฎูุงููุฏ",
                ]
            )

            with codecs.open(verbs_file, encoding="utf8") as verbs_file:
                self.verbs = list(
                    reversed([verb.strip() for verb in verbs_file if verb])
                )
                self.bons = set([verb.split("#")[0] for verb in self.verbs])
                self.verbe = set(
                    [bon + "ู" for bon in self.bons]
                    + ["ู" + bon + "ู" for bon in self.bons]
                )

    def tokenize(self, text):
        """ุชูฺฉูโูุง ูุชู ุฑุง ุงุณุชุฎุฑุงุฌ ูโฺฉูุฏ.

        Examples:
                >>> tokenizer = WordTokenizer()
                >>> tokenizer.tokenize('ุงู ุฌููู (ุฎู) ูพฺุฏู ูุณุช!!!')
                ['ุงู', 'ุฌููู', '(', 'ุฎู', ')', 'ูพฺุฏู', 'ูุณุช', '!!!']

                >>> tokenizer.tokenize('ูุณุฎู 0.5 ุฏุฑ ุณุงุนุช 22:00 ุชูุฑุงูุ1396.')
                ['ูุณุฎู', '0.5', 'ุฏุฑ', 'ุณุงุนุช', '22:00', 'ุชูุฑุงู', 'ุ', '1396', '.']

                >>> tokenizer = WordTokenizer(join_verb_parts=False)
                >>> print(' '.join(tokenizer.tokenize('ุณูุงู.')))
                ุณูุงู .

                >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_links=True)
                >>> print(' '.join(tokenizer.tokenize('ุฏุฑ ูุทุฑ ูฺฉ ุดุฏ https://t.co/tZOurPSXzi https://t.co/vtJtwsRebP')))
                ุฏุฑ ูุทุฑ ูฺฉ ุดุฏ LINK LINK

                >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_IDs=True, replace_numbers=True)
                >>> print(' '.join(tokenizer.tokenize('ุฒูุฒูู ด.ธ ุฑุดุชุฑ ุฏุฑ ูุฌุฏฺฉ ฺฉุฑูุงู @bourse24ir')))
                ุฒูุฒูู NUMF ุฑุดุชุฑ ุฏุฑ ูุฌุฏฺฉ ฺฉุฑูุงู ID

                >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_hashtags=True, replace_numbers=True, separate_emoji=True)
                >>> print(' '.join(tokenizer.tokenize('๐ุนุฑุถู ุจููฺฉ 17 ุฏุฑุตุฏ #ูุง_ูุจ ุจู ููุช')))
                ๐ ุนุฑุถู ุจููฺฉ NUM2 ุฏุฑุตุฏ TAG ูุง ูุจ ุจู ููุช

                >>> tokenizer = WordTokenizer(join_verb_parts=False, separate_emoji=True)
                >>> print(' '.join(tokenizer.tokenize('ุฏฺฏู ูุฎูุงู ุชุฑฺฉ ุชุญุตู ฺฉูู ๐๐๐')))
                ุฏฺฏู ูุฎูุงู ุชุฑฺฉ ุชุญุตู ฺฉูู ๐ ๐ ๐

        Args:
                text (str): ูุชู ฺฉู ุจุงุฏ ุชูฺฉูโูุง ุขู ุงุณุชุฎุฑุงุฌ ุดูุฏ.

        Returns:
                (List[str]): ูุณุช ุชูฺฉูโูุง ุงุณุชุฎุฑุงุฌโุดุฏู.
        """

        if self.separate_emoji:
            text = self.emoji_pattern.sub(self.emoji_repl, text)
        if self.replace_emails:
            text = self.email_pattern.sub(self.email_repl, text)
        if self.replace_links:
            text = self.link_pattern.sub(self.link_repl, text)
        if self.replace_IDs:
            text = self.id_pattern.sub(self.id_repl, text)
        if self.replace_hashtags:
            text = self.hashtag_pattern.sub(self.hashtag_repl, text)
        if self.replace_numbers:
            text = self.number_int_pattern.sub(self.number_int_repl, text)
            text = self.number_float_pattern.sub(self.number_float_repl, text)

        text = self.pattern.sub(r" \1 ", text.replace("\n", " ").replace("\t", " "))

        tokens = [word for word in text.split(" ") if word]
        if self._join_verb_parts:
            tokens = self.join_verb_parts(tokens)
        return tokens

    def join_verb_parts(self, tokens):
        """ุงูุนุงู ฺูุฏุจุฎุด ุฑุง ุจู ูู ูโฺุณุจุงูุฏ.

        Examples:
            >>> tokenizer = WordTokenizer()
            >>> tokenizer.join_verb_parts(['ุฎูุงูุฏ', 'ุฑูุช'])
            ['ุฎูุงูุฏ_ุฑูุช']
            >>> tokenizer.join_verb_parts(['ุฑูุชู', 'ุงุณุช'])
            ['ุฑูุชู_ุงุณุช']
            >>> tokenizer.join_verb_parts(['ฺฏูุชู', 'ุดุฏู', 'ุงุณุช'])
            ['ฺฏูุชู_ุดุฏู_ุงุณุช']
            >>> tokenizer.join_verb_parts(['ฺฏูุชู', 'ุฎูุงูุฏ', 'ุดุฏ'])
            ['ฺฏูุชู_ุฎูุงูุฏ_ุดุฏ']
            >>> tokenizer.join_verb_parts(['ุฎุณุชู', 'ุดุฏุฏ'])
            ['ุฎุณุชู', 'ุดุฏุฏ']
        Args:
            tokens (List[str]): ูุณุช ฺฉููุงุช ฺฉ ูุนู ฺูุฏุจุฎุด.
        Returns:
            (List[str]): ูุณุช ุงุฒ ุงูุนุงู ฺูุฏุจุฎุด ฺฉู ุฏุฑ ุตูุฑุช ูุฒูู ุจุฎุดโูุง ุขู ุจุง ฺฉุงุฑุงฺฉุชุฑ ุฎุท ุฒุฑ ุจู ูู ฺุณุจุงูุฏู_ุดุฏู_ุงุณุช.
        """

        result = [""]
        for token in reversed(tokens):
            if token in self.before_verbs or (
                result[-1] in self.after_verbs and token in self.verbe
            ):
                result[-1] = token + "_" + result[-1]
            else:
                result.append(token)
        return list(reversed(result[1:]))
