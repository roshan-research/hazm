"""Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ø´Ø§Ù…Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ùˆ ØªÙˆØ§Ø¨Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§ØªÙ Ù…ØªÙ† Ø§Ø³Øª.

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…Ù„Ø§ØªØŒ Ø§Ø² ØªØ§Ø¨Ø¹ [SentenceTokenizer()][hazm.SentenceTokenizer]
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.

"""


import re
from pathlib import Path
from typing import List

from nltk.tokenize.api import TokenizerI

from hazm import default_verbs
from hazm import default_words
from hazm import words_list


class WordTokenizer(TokenizerI):
    """Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø´Ø§Ù…Ù„ ØªÙˆØ§Ø¨Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§ØªÙ Ù…ØªÙ† Ø§Ø³Øª.

    Args:
        words_file: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø­Ø§ÙˆÛŒ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª.
            Ù‡Ø¶Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ù†Ø¸ÙˆØ± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³ØªØ› Ø¨Ø§
            Ø§ÛŒÙ† Ø­Ø§Ù„ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙØ§ÛŒÙ„ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ø¢Ú¯Ø§Ù‡ÛŒ Ø§Ø²
            Ø³Ø§Ø®ØªØ§Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.
        verbs_file: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø­Ø§ÙˆÛŒ Ø§ÙØ¹Ø§Ù„.
            Ù‡Ø¶Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ ÙØ§ÛŒÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ù†Ø¸ÙˆØ± Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³ØªØ› Ø¨Ø§
            Ø§ÛŒÙ† Ø­Ø§Ù„ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙØ§ÛŒÙ„ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø¹Ø±ÙÛŒ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ø¢Ú¯Ø§Ù‡ÛŒ Ø§Ø²
            Ø³Ø§Ø®ØªØ§Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.
        join_verb_parts: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø§ÙØ¹Ø§Ù„ Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ Ø±Ø§ Ø¨Ø§ Ø®Ø· Ø²ÛŒØ± Ø¨Ù‡ Ù‡Ù… Ù…ÛŒâ€ŒÚ†Ø³Ø¨Ø§Ù†Ø¯Ø› Ù…Ø«Ù„Ø§Ù‹ Â«Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³ØªÂ» Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Â«Ú¯ÙØªÙ‡_Ø´Ø¯Ù‡_Ø§Ø³ØªÂ» Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        separate_emoji: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø§Ù…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ ÛŒÚ© ÙØ§ØµÙ„Ù‡ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        replace_links: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù„Ù…Ù‡Ù” `LINK` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        replace_ids: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù„Ù…Ù‡Ù” `ID` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        replace_emails: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø§ Ú©Ù„Ù…Ù‡Ù” `EMAILâ€` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        replace_numbers: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø§`NUMF` Ùˆ Ø§Ø¹Ø¯Ø§Ø¯ ØµØ­ÛŒØ­ Ø±Ø§ Ø¨Ø§` NUM` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¯Ø± Ø§Ø¹Ø¯Ø§Ø¯ ØºÛŒØ±Ø§Ø¹Ø´Ø§Ø±ÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ù‚Ø§Ù… Ù†ÛŒØ² Ø¬Ù„ÙˆÛŒ `NUM` Ù…ÛŒâ€ŒØ¢ÛŒØ¯.
        replace_hashtags: Ø§Ú¯Ø± `True` Ø¨Ø§Ø´Ø¯ Ø¹Ù„Ø§Ù…Øª `#` Ø±Ø§ Ø¨Ø§ `TAG` Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

    """

    def __init__(
        self: "WordTokenizer",
        words_file: str = default_words,
        verbs_file: str = default_verbs,
        join_verb_parts: bool = True,
        separate_emoji: bool = False,
        replace_links: bool = False,
        replace_ids: bool = False,
        replace_emails: bool = False,
        replace_numbers: bool = False,
        replace_hashtags: bool = False,
    ) -> None:
        self._join_verb_parts = join_verb_parts
        self.separate_emoji = separate_emoji
        self.replace_links = replace_links
        self.replace_ids = replace_ids
        self.replace_emails = replace_emails
        self.replace_numbers = replace_numbers
        self.replace_hashtags = replace_hashtags

        self.pattern = re.compile(r'([ØŸ!?]+|[\d.:]+|[:.ØŒØ›Â»\])}"Â«\[({/\\])')  # TODO \d
        self.emoji_pattern = re.compile(
            "["
            "\U0001f600-\U0001f64f"  # emoticons
            "\U0001f300-\U0001f5ff"  # symbols & pictographs
            "\U0001f4cc\U0001f4cd"  # other emojis
            "]",
            flags=re.UNICODE,
        )
        self.emoji_repl = r"\g<0> "
        self.id_pattern = re.compile(r"(?<![\w._])(@[\w_]+)")
        self.id_repl = r" ID "
        self.link_pattern = re.compile(
            r"((https?|ftp)://)?(?<!@)(([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,})[-\w@:%_.+/~#?=&]*",
        )
        self.link_repl = r" LINK "
        self.email_pattern = re.compile(
            r"[a-zA-Z0-9._+-]+@([a-zA-Z0-9-]+\.)+[A-Za-z]{2,}",
        )
        self.email_repl = r" EMAIL "

        # 'Ù«' is the decimal separator and 'Ù¬' is the thousands separator
        self.number_int_pattern = re.compile(
            r"\b(?<![\dÛ°-Û¹][.Ù«Ù¬,])([\dÛ°-Û¹]+)(?![.Ù«Ù¬,][\dÛ°-Û¹])\b",
        )
        self.number_int_repl = lambda m: " NUM" + str(len(m.group(1))) + " "
        self.number_float_pattern = re.compile(
            r"\b(?<!\.)([\dÛ°-Û¹,Ù¬]+[.Ù«Ù¬][\dÛ°-Û¹]+)\b(?!\.)",
        )
        self.number_float_repl = r" NUMF "

        self.hashtag_pattern = re.compile(r"#(\S+)")
        # NOTE: python2.7 does not support unicodes with \w

        self.hashtag_repl = lambda m: "TAG " + m.group(1).replace("_", " ")

        self.words = {item[0]: (item[1], item[2]) for item in words_list(words_file)}

        if join_verb_parts:
            self.after_verbs = {
                "Ø§Ù…",
                "Ø§ÛŒ",
                "Ø§Ø³Øª",
                "Ø§ÛŒÙ…",
                "Ø§ÛŒØ¯",
                "Ø§Ù†Ø¯",
                "Ø¨ÙˆØ¯Ù…",
                "Ø¨ÙˆØ¯ÛŒ",
                "Ø¨ÙˆØ¯",
                "Ø¨ÙˆØ¯ÛŒÙ…",
                "Ø¨ÙˆØ¯ÛŒØ¯",
                "Ø¨ÙˆØ¯Ù†Ø¯",
                "Ø¨Ø§Ø´Ù…",
                "Ø¨Ø§Ø´ÛŒ",
                "Ø¨Ø§Ø´Ø¯",
                "Ø¨Ø§Ø´ÛŒÙ…",
                "Ø¨Ø§Ø´ÛŒØ¯",
                "Ø¨Ø§Ø´Ù†Ø¯",
                "Ø´Ø¯Ù‡_Ø§Ù…",
                "Ø´Ø¯Ù‡_Ø§ÛŒ",
                "Ø´Ø¯Ù‡_Ø§Ø³Øª",
                "Ø´Ø¯Ù‡_Ø§ÛŒÙ…",
                "Ø´Ø¯Ù‡_Ø§ÛŒØ¯",
                "Ø´Ø¯Ù‡_Ø§Ù†Ø¯",
                "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù…",
                "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒ",
                "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯",
                "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒÙ…",
                "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒØ¯",
                "Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù†Ø¯",
                "Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù…",
                "Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒ",
                "Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ø¯",
                "Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒÙ…",
                "Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒØ¯",
                "Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù†Ø¯",
                "Ù†Ø´Ø¯Ù‡_Ø§Ù…",
                "Ù†Ø´Ø¯Ù‡_Ø§ÛŒ",
                "Ù†Ø´Ø¯Ù‡_Ø§Ø³Øª",
                "Ù†Ø´Ø¯Ù‡_Ø§ÛŒÙ…",
                "Ù†Ø´Ø¯Ù‡_Ø§ÛŒØ¯",
                "Ù†Ø´Ø¯Ù‡_Ø§Ù†Ø¯",
                "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù…",
                "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒ",
                "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯",
                "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒÙ…",
                "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯ÛŒØ¯",
                "Ù†Ø´Ø¯Ù‡_Ø¨ÙˆØ¯Ù†Ø¯",
                "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù…",
                "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒ",
                "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ø¯",
                "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒÙ…",
                "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´ÛŒØ¯",
                "Ù†Ø´Ø¯Ù‡_Ø¨Ø§Ø´Ù†Ø¯",
                "Ø´ÙˆÙ…",
                "Ø´ÙˆÛŒ",
                "Ø´ÙˆØ¯",
                "Ø´ÙˆÛŒÙ…",
                "Ø´ÙˆÛŒØ¯",
                "Ø´ÙˆÙ†Ø¯",
                "Ø´Ø¯Ù…",
                "Ø´Ø¯ÛŒ",
                "Ø´Ø¯",
                "Ø´Ø¯ÛŒÙ…",
                "Ø´Ø¯ÛŒØ¯",
                "Ø´Ø¯Ù†Ø¯",
                "Ù†Ø´ÙˆÙ…",
                "Ù†Ø´ÙˆÛŒ",
                "Ù†Ø´ÙˆØ¯",
                "Ù†Ø´ÙˆÛŒÙ…",
                "Ù†Ø´ÙˆÛŒØ¯",
                "Ù†Ø´ÙˆÙ†Ø¯",
                "Ù†Ø´Ø¯Ù…",
                "Ù†Ø´Ø¯ÛŒ",
                "Ù†Ø´Ø¯",
                "Ù†Ø´Ø¯ÛŒÙ…",
                "Ù†Ø´Ø¯ÛŒØ¯",
                "Ù†Ø´Ø¯Ù†Ø¯",
                "Ù…ÛŒâ€ŒØ´ÙˆÙ…",
                "Ù…ÛŒâ€ŒØ´ÙˆÛŒ",
                "Ù…ÛŒâ€ŒØ´ÙˆØ¯",
                "Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…",
                "Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯",
                "Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯",
                "Ù…ÛŒâ€ŒØ´Ø¯Ù…",
                "Ù…ÛŒâ€ŒØ´Ø¯ÛŒ",
                "Ù…ÛŒâ€ŒØ´Ø¯",
                "Ù…ÛŒâ€ŒØ´Ø¯ÛŒÙ…",
                "Ù…ÛŒâ€ŒØ´Ø¯ÛŒØ¯",
                "Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯",
                "Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ…",
                "Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒ",
                "Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯",
                "Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…",
                "Ù†Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯",
                "Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯",
                "Ù†Ù…ÛŒâ€ŒØ´Ø¯Ù…",
                "Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒ",
                "Ù†Ù…ÛŒâ€ŒØ´Ø¯",
                "Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒÙ…",
                "Ù†Ù…ÛŒâ€ŒØ´Ø¯ÛŒØ¯",
                "Ù†Ù…ÛŒâ€ŒØ´Ø¯Ù†Ø¯",
                "Ø®ÙˆØ§Ù‡Ù…_Ø´Ø¯",
                "Ø®ÙˆØ§Ù‡ÛŒ_Ø´Ø¯",
                "Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯",
                "Ø®ÙˆØ§Ù‡ÛŒÙ…_Ø´Ø¯",
                "Ø®ÙˆØ§Ù‡ÛŒØ¯_Ø´Ø¯",
                "Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡Ù…_Ø´Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡ÛŒ_Ø´Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡ÛŒÙ…_Ø´Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯_Ø´Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯_Ø´Ø¯",
            }

            self.before_verbs = {
                "Ø®ÙˆØ§Ù‡Ù…",
                "Ø®ÙˆØ§Ù‡ÛŒ",
                "Ø®ÙˆØ§Ù‡Ø¯",
                "Ø®ÙˆØ§Ù‡ÛŒÙ…",
                "Ø®ÙˆØ§Ù‡ÛŒØ¯",
                "Ø®ÙˆØ§Ù‡Ù†Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡Ù…",
                "Ù†Ø®ÙˆØ§Ù‡ÛŒ",
                "Ù†Ø®ÙˆØ§Ù‡Ø¯",
                "Ù†Ø®ÙˆØ§Ù‡ÛŒÙ…",
                "Ù†Ø®ÙˆØ§Ù‡ÛŒØ¯",
                "Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯",
            }

            with Path.open(verbs_file, encoding="utf8") as verbs_file:
                self.verbs = list(
                    reversed([verb.strip() for verb in verbs_file if verb]),
                )
                self.bons = {verb.split("#")[0] for verb in self.verbs}
                self.verbe = set(
                    [bon + "Ù‡" for bon in self.bons]
                    + ["Ù†" + bon + "Ù‡" for bon in self.bons],
                )

    def tokenize(self: "WordTokenizer", text: str) -> List[str]:
        """ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        Examples:
            >>> tokenizer = WordTokenizer()
            >>> tokenizer.tokenize('Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ (Ø®ÛŒÙ„ÛŒ) Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù†ÛŒØ³Øª!!!')
            ['Ø§ÛŒÙ†', 'Ø¬Ù…Ù„Ù‡', '(', 'Ø®ÛŒÙ„ÛŒ', ')', 'Ù¾ÛŒÚ†ÛŒØ¯Ù‡', 'Ù†ÛŒØ³Øª', '!!!']
            >>> tokenizer = WordTokenizer(join_verb_parts=False)
            >>> print(' '.join(tokenizer.tokenize('Ø³Ù„Ø§Ù….')))
            Ø³Ù„Ø§Ù… .
            >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_links=True)
            >>> print(' '.join(tokenizer.tokenize('Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ https://t.co/tZOurPSXzi https://t.co/vtJtwsRebP')))
            Ø¯Ø± Ù‚Ø·Ø± Ù‡Ú© Ø´Ø¯ LINK LINK
            >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_ids=True, replace_numbers=True)
            >>> print(' '.join(tokenizer.tokenize('Ø²Ù„Ø²Ù„Ù‡ Û´.Û¸ Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† @bourse24ir')))
            Ø²Ù„Ø²Ù„Ù‡ NUMF Ø±ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ù‡Ø¬Ø¯Ú© Ú©Ø±Ù…Ø§Ù† ID
            >>> tokenizer = WordTokenizer(join_verb_parts=False, separate_emoji=True)
            >>> print(' '.join(tokenizer.tokenize('Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ğŸ˜‚ğŸ˜‚ğŸ˜‚')))
            Ø¯ÛŒÚ¯Ù‡ Ù…ÛŒØ®ÙˆØ§Ù… ØªØ±Ú© ØªØ­ØµÛŒÙ„ Ú©Ù†Ù… ğŸ˜‚ ğŸ˜‚ ğŸ˜‚

        Args:
            text: Ù…ØªÙ†ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯.

        Returns:
            Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡.

        """
        # >>> tokenizer.tokenize('Ù†Ø³Ø®Ù‡ 0.5 Ø¯Ø± Ø³Ø§Ø¹Øª 22:00 ØªÙ‡Ø±Ø§Ù†ØŒ1396.')
        # >>> tokenizer = WordTokenizer(join_verb_parts=False, replace_hashtags=True, replace_numbers=True, separate_emoji=True)
        # >>> print(' '.join(tokenizer.tokenize('ğŸ“Ø¹Ø±Ø¶Ù‡ Ø¨Ù„ÙˆÚ© 17 Ø¯Ø±ØµØ¯ÛŒ #Ù‡Ø§ÛŒ_ÙˆØ¨ Ø¨Ù‡ Ù‚ÛŒÙ…Øª')))
        # ğŸ“ Ø¹Ø±Ø¶Ù‡ Ø¨Ù„ÙˆÚ© NUM2 Ø¯Ø±ØµØ¯ÛŒ TAG Ù‡Ø§ÛŒ ÙˆØ¨ Ø¨Ù‡ Ù‚ÛŒÙ…Øª

        if self.separate_emoji:
            text = self.emoji_pattern.sub(self.emoji_repl, text)
        if self.replace_emails:
            text = self.email_pattern.sub(self.email_repl, text)
        if self.replace_links:
            text = self.link_pattern.sub(self.link_repl, text)
        if self.replace_ids:
            text = self.id_pattern.sub(self.id_repl, text)
        if self.replace_hashtags:
            text = self.hashtag_pattern.sub(self.hashtag_repl, text)
        if self.replace_numbers:
            text = self.number_int_pattern.sub(self.number_int_repl, text)
            text = self.number_float_pattern.sub(self.number_float_repl, text)

        text = self.pattern.sub(r" \1 ", text.replace("\n", " ").replace("\t", " "))

        tokens = [word for word in text.split(" ") if word]
        if self._join_verb_parts:
            tokens = self.join_verb_parts(tokens)
        return tokens

    def join_verb_parts(self: "WordTokenizer", tokens: List[str]) -> List[str]:
        """Ø§ÙØ¹Ø§Ù„ Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ Ø±Ø§ Ø¨Ù‡ Ù‡Ù… Ù…ÛŒâ€ŒÚ†Ø³Ø¨Ø§Ù†Ø¯.

        Examples:
            >>> tokenizer = WordTokenizer()
            >>> tokenizer.join_verb_parts(['Ø®ÙˆØ§Ù‡Ø¯', 'Ø±ÙØª'])
            ['Ø®ÙˆØ§Ù‡Ø¯_Ø±ÙØª']
            >>> tokenizer.join_verb_parts(['Ø±ÙØªÙ‡', 'Ø§Ø³Øª'])
            ['Ø±ÙØªÙ‡_Ø§Ø³Øª']
            >>> tokenizer.join_verb_parts(['Ú¯ÙØªÙ‡', 'Ø´Ø¯Ù‡', 'Ø§Ø³Øª'])
            ['Ú¯ÙØªÙ‡_Ø´Ø¯Ù‡_Ø§Ø³Øª']
            >>> tokenizer.join_verb_parts(['Ú¯ÙØªÙ‡', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ø´Ø¯'])
            ['Ú¯ÙØªÙ‡_Ø®ÙˆØ§Ù‡Ø¯_Ø´Ø¯']
            >>> tokenizer.join_verb_parts(['Ø®Ø³ØªÙ‡', 'Ø´Ø¯ÛŒØ¯'])
            ['Ø®Ø³ØªÙ‡', 'Ø´Ø¯ÛŒØ¯']

        Args:
            tokens: Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª ÛŒÚ© ÙØ¹Ù„ Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ.

        Returns:
            Ù„ÛŒØ³Øª Ø§Ø² Ø§ÙØ¹Ø§Ù„ Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ Ú©Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ù„Ø²ÙˆÙ… Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ø¨Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø®Ø· Ø²ÛŒØ± Ø¨Ù‡ Ù‡Ù… Ú†Ø³Ø¨Ø§Ù†Ø¯Ù‡_Ø´Ø¯Ù‡_Ø§Ø³Øª.

        """
        if len(tokens) == 1:
            return tokens

        result = [""]
        for token in reversed(tokens):
            if token in self.before_verbs or (
                result[-1] in self.after_verbs and token in self.verbe
            ):
                result[-1] = token + "_" + result[-1]
            else:
                result.append(token)
        return list(reversed(result[1:]))
